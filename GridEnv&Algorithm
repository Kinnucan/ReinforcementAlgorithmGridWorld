{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld():\n",
    "    def __init__(self):\n",
    "        self.state_space = [(x, y) for x in range(1,5) \n",
    "                                for y in range(1,4)]\n",
    "        self.action_space = [\"N\", \"S\", \"E\", \"W\"]\n",
    "        self.action_probs = {\"N\": {\"N\": 0.8, \"E\": 0.1, \"W\": 0.1}, \"S\": {\"S\": 0.8, \"E\": 0.1, \"W\": 0.1}, \n",
    "                             \"E\": {\"E\": 0.8, \"N\": 0.1, \"S\": 0.1}, \"W\": {\"W\": 0.8, \"N\": 0.1, \"S\": 0.1}}\n",
    "        \n",
    "#   Get the next state and the probability of transitioning to that state based off of\n",
    "#   a current state, an action, and the probability of that action\n",
    "    def get_next_state(self, state, action_prob, next_s, prob_next_s, a):\n",
    "        if (a == \"N\" and state[1] + 1 <= 3 and (state[0], state[1] + 1) != (2,2)):\n",
    "                next_s.append((state[0], state[1] + 1))\n",
    "                prob_next_s.append(action_prob)\n",
    "        elif (a == \"S\" and state[1] - 1 >= 1 and (state[0], state[1] - 1) != (2,2)):\n",
    "                next_s.append((state[0], state[1] - 1))\n",
    "                prob_next_s.append(action_prob)\n",
    "        elif (a == \"E\" and state[0] + 1 <= 4 and (state[0] + 1, state[1]) != (2,2)):\n",
    "                next_s.append((state[0] + 1, state[1]))\n",
    "                prob_next_s.append(action_prob)\n",
    "        elif (a == \"W\" and state[0] - 1 >= 1 and (state[0] - 1, state[1]) != (2,2)):\n",
    "                next_s.append((state[0] - 1, state[1])) \n",
    "                prob_next_s.append(action_prob)\n",
    "        else:\n",
    "            next_s.append(state)\n",
    "            prob_next_s.append(action_prob)\n",
    "        return next_s, prob_next_s\n",
    "\n",
    "#   Construct and return the list of the next possible states and the list of their\n",
    "#   probabilities based on current state and action. Also checks it current state\n",
    "#   is either terminal or a wall, and returns different lists accordingly\n",
    "    def get_next_states_probs(self, state, action):\n",
    "        next_s = []\n",
    "        prob_next_s = []\n",
    "        if (state == (4,3) or state == (4,2)):\n",
    "            next_s.append(state)\n",
    "            prob_next_s.append(1)\n",
    "        elif (state == (2,2)):\n",
    "            return [], []\n",
    "        else:\n",
    "            action_probs = self.action_probs[action]\n",
    "            for a in action_probs:\n",
    "                next_s, prob_next_s = self.get_next_state(state, action_probs[a], next_s, prob_next_s, a)\n",
    "        return next_s, prob_next_s\n",
    "    \n",
    "#   Gets and returns the transition reward. Reward depends on whether current\n",
    "#   state is terminal or non-terminal and if user has specified a staying alive\n",
    "#   reward greater than 0\n",
    "    def get_transition_reward(self, reward, state):\n",
    "        if (state == (4,3)):\n",
    "            return 1\n",
    "        elif (state == (4,2)):\n",
    "            return -1\n",
    "        else:\n",
    "            return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds the optimal policy for the grid world\n",
    "def find_optimal_policy(env, gamma, alive_reward, num_iter):\n",
    "    v = {s: 0 for s in env.state_space}\n",
    "    action_values = {state: {\"N\": 0, \"S\": 0, \"E\": 0, \"W\": 0} for state in env.state_space}\n",
    "    for i in range(num_iter):\n",
    "        v_prime = {s: 0 for s in env.state_space}\n",
    "        for s in env.state_space:\n",
    "            if (v[s] != 1.0 and v[s] != -1.0):\n",
    "                q_max = float('-inf')\n",
    "                for a in env.action_space:\n",
    "                    q_sa = 0\n",
    "                    next_s, prob_next_s = env.get_next_states_probs(s, a)\n",
    "                    for i in range(len(next_s)):\n",
    "                        s_prime = next_s[i]\n",
    "                        prob_next_s_r = prob_next_s[i]\n",
    "                        r = env.get_transition_reward(alive_reward, s)\n",
    "                        q_sa += prob_next_s_r * (r + gamma * v[s_prime])\n",
    "                    action_values[s][a] = q_sa\n",
    "                    q_max = max(q_max, q_sa)\n",
    "                v_prime[s] = q_max\n",
    "            else:\n",
    "                v_prime[s] = v[s]\n",
    "        v = v_prime\n",
    "    print(\"State Values:\")\n",
    "    for k in v.keys():\n",
    "        print(k,':', round(v[k], 2))\n",
    "    print(\"\")\n",
    "    print(\"Action Values:\")\n",
    "    for sa in action_values:\n",
    "        cell_values = str(sa)\n",
    "        cell_values += \" \"\n",
    "        for ssa in action_values[sa]:\n",
    "            cell_values += str(ssa)\n",
    "            cell_values += \":\"\n",
    "            cell_values += str(round(action_values[sa][ssa], 2))\n",
    "            cell_values += \" \"\n",
    "        print(cell_values)\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Values:\n",
      "(1, 1) : 0.49\n",
      "(1, 2) : 0.57\n",
      "(1, 3) : 0.64\n",
      "(2, 1) : 0.43\n",
      "(2, 2) : 0\n",
      "(2, 3) : 0.74\n",
      "(3, 1) : 0.48\n",
      "(3, 2) : 0.57\n",
      "(3, 3) : 0.85\n",
      "(4, 1) : 0.28\n",
      "(4, 2) : -1.0\n",
      "(4, 3) : 1.0\n",
      "\n",
      "Action Values:\n",
      "(1, 1) N:0.49 S:0.44 E:0.41 W:0.45 \n",
      "\n",
      "(1, 2) N:0.57 S:0.46 E:0.51 W:0.51 \n",
      "\n",
      "(1, 3) N:0.59 S:0.53 E:0.64 W:0.57 \n",
      "\n",
      "(2, 1) N:0.4 S:0.4 E:0.42 W:0.43 \n",
      "\n",
      "(2, 2) N:0 S:0 E:0 W:0 \n",
      "\n",
      "(2, 3) N:0.67 S:0.67 E:0.74 W:0.6 \n",
      "\n",
      "(3, 1) N:0.48 S:0.41 E:0.29 W:0.4 \n",
      "\n",
      "(3, 2) N:0.57 S:0.3 E:-0.6 W:0.53 \n",
      "\n",
      "(3, 3) N:0.77 S:0.57 E:0.85 W:0.66 \n",
      "\n",
      "(4, 1) N:-0.65 S:0.27 E:0.13 W:0.28 \n",
      "\n",
      "(4, 2) N:-1.0 S:-1.0 E:-1.0 W:-1.0 \n",
      "\n",
      "(4, 3) N:1.0 S:1.0 E:1.0 W:1.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "grid = GridWorld()\n",
    "find_optimal_policy(grid, 0.9, 0, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
